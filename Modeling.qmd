---
title: "Modeling"
format: html
editor: visual
---

## About the Models
The goal of these models is to see how different lifestyle choices affect the likelihood of an American having diabetes. This will include diet, exercise, and smoking habits.

## Preparing Data for Models
First I will read in the data again, subsetting it the same it was in the EDA.
```{r}
## reading in data again
library(tidyverse)
temp_data <- read_csv("diabetes_binary_health_indicators_BRFSS2015.csv", col_names = TRUE)

## subsetting the data
dia_data <- temp_data |>
  select(Diabetes_binary, BMI, Smoker, PhysActivity, Fruits, Veggies, HvyAlcoholConsump,MentHlth, PhysHlth) |>
  drop_na() |>
  # turning binary vars into factors
  mutate(Diabetes_binary = factor(Diabetes_binary, levels = c(0, 1), labels = c("no diabetes", "diabetes"))) |>
  mutate(Smoker = factor(Smoker, levels = c(0, 1), labels = c("has not smoked 5 packs", "has smoked 5 packs"))) |>
  mutate(PhysActivity = factor(PhysActivity, levels = c(0, 1), labels = c("no exercise last 30 days", "has exercised last 30 days"))) |>
  mutate(Fruits = factor(Fruits, levels = c(0, 1), labels = c("does not eat fruit daily", "eats fruit daily"))) |>
  mutate(Veggies = factor(Veggies, levels = c(0, 1), labels = c("does not eat veggies daily", "eats veggies daily"))) |>
  mutate(HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0, 1), labels = c("not a heavy drinker", "heavy drinker")))

names_vec <- c("dia_binary","bmi", "smoker", "exercise", "fruits", "veggies", "alcohol", "ment_hlth", "phys_hlth")
names(dia_data) <- names_vec
```

Now I will do a 70/30 split for the training/testing data sets.
```{r}
## splitting the data
library(tidymodels)
set.seed(123)
dia_split <- initial_split(dia_data, 0.7)
dia_train <- training(dia_split)
dia_test <- testing(dia_split)

## set up for cross validation
dia_5_fold <- vfold_cv(dia_train, 5)
```

## Classification Tree
A classification tree is a model that splits up predictor space into regions. Then a prediction is made based on which bin an observation ends up in. The goal is to predict group membership, usually using the most prevalent class in a region as the prediction.
```{r}
set.seed(123)
## creating the tree recipe
tree_rec <-
  recipe(dia_binary ~ ., data = dia_data) |>
  step_dummy(smoker, exercise, fruits, veggies, alcohol) |>
  step_normalize(all_numeric(), -all_outcomes())

## defining the tree model
tree_mod <- decision_tree(tree_depth = tune(),
                          min_n = 20,
                          cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")

## creating the tree workflow
tree_wkf <- workflow() |>
  add_recipe(tree_rec) |>
  add_model(tree_mod)

## creating the tree tuning grid
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = c(10, 5))

## fitting the tree model
tree_fits <- tree_wkf |> 
  tune_grid(resamples = dia_5_fold,
            grid = tree_grid,
            metrics = metric_set(mn_log_loss))
```

```{r}
set.seed(123)
## selecting the best model
tree_best_params <- tree_fits |>
  select_best(metric = "mn_log_loss")

tree_final_wkf <- tree_wkf |>
  finalize_workflow(tree_best_params)

tree_final_fit <- tree_final_wkf |>
  last_fit(dia_split, metrics = metric_set(mn_log_loss))

tree_best_fit <- tree_final_fit |>
  collect_metrics()

tree_best_fit
```

## Random Forest
A random forest is a model that combines multiple classification trees that are created from boostrap samples. It uses the aggregate from the trees to find an average across them all. Additionally, it does not use all predictors in each step. It can be a useful alternative to a single classification tree as those are prone to over fitting. As a result, it may produce a better model.
```{r}
set.seed(123)
library(ranger)

## defining the rf model
rf_spec <- rand_forest(mtry = tune()) |>
  set_engine("ranger", importance = "impurity") |>
  set_mode("classification")

## creating the rf workflow
rf_wkf <- workflow() |>
  add_recipe(tree_rec) |>
  add_model(rf_spec)

## fitting the rf model
rf_fits <- rf_wkf |> 
  tune_grid(resamples = dia_5_fold,
            grid = 5,
            metrics = metric_set(mn_log_loss))
```

```{r}
set.seed(123)
## selecting the best model
rf_best_params <- rf_fits |>
  select_best(metric = "mn_log_loss")

rf_final_wkf <- rf_wkf |>
  finalize_workflow(rf_best_params)

rf_final_fit <- rf_final_wkf |>
  last_fit(dia_split, metrics = metric_set(mn_log_loss))

rf_best_fit <- rf_final_fit |>
  collect_metrics()

rf_best_fit
```
```{r}
set.seed(123)
best_model <- tree_wkf |>
  finalize_workflow(tree_best_params) |>
  fit(dia_data)

new_data <- tibble(
  bmi = 40,
  smoker = factor("has not smoked 5 packs", levels = c("has not smoked 5 packs", "has smoked 5 packs")),
  exercise = factor("no exercise last 30 days", levels = c("no exercise last 30 days", "has exercised last 30 days")),
  fruits = factor("does not eat fruit daily", levels = c("does not eat fruit daily", "eats fruit daily")),
  veggies = factor("does not eat veggies daily", levels = c("does not eat veggies daily", "eats veggies daily")),
  alcohol = factor("heavy drinker", levels = c("not a heavy drinker", "heavy drinker"))
)


final_pred <- predict(best_model, dia_data, type = "prob")
final_pred |>
  filter(.pred_diabetes > 0.5)
```


The Random Forest is the better model in the case as it produces a smaller log loss.